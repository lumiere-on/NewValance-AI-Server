from __future__ import annotations
import time, logging, sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict

from selenium import webdriver
from selenium.webdriver.common.by import By

from app.core.logger import logger

SECTIONS = {
    "100": 1, #ì •ì¹˜
    "101": 2, #ê²½ì œ
    "102": 6, #ì‚¬íšŒ
    "103": 4, #ìƒí™œ/ë¬¸í™”
    "104": 3, #ì„¸ê³„
    "105": 5, #IT/ê³¼í•™
}
def _get_driver(headless: bool = True) -> webdriver.Chrome:
    opts = webdriver.ChromeOptions()
    if headless:
        opts.add_argument("--headless=new")   # êµ¬ë²„ì „ Chromeì´ë©´ "--headless"ë¡œ ë³€ê²½
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("lang=ko_KR")
    return webdriver.Chrome(options=opts)     # Selenium 4.20+ â†’ driver auto-download

def crawl_headlines(sections: List[str]|None=None) -> List[Dict]:
    """
    ì„¹ì…˜ë³„ í—¤ë“œë¼ì¸(ì œëª©+URL)ë§Œ ìš°ì„  ìˆ˜ì§‘
    """
    drv = _get_driver()
    date_str = datetime.utcnow().strftime("%Y%m%d")
    res, base, idx = [], "https://news.naver.com/section/", 0
    try:
        for sec in sections or SECTIONS:
            drv.get(base + sec); 
            time.sleep(1.5)
            for art in drv.find_elements(By.CSS_SELECTOR,"a.sa_text_title"):
                res.append({
                    "index": idx,
                    "date": date_str,
                    "section": SECTIONS.get(sec,sec),
                    "url": art.get_attribute("href"),
                    "title": art.find_element(By.TAG_NAME,"strong").text.strip()
                }); 
                idx+=1
        logger.info(f"[HEAD] total {len(res)} collected")
        return res
    finally: drv.quit()

def enrich_articles(news_list: List[Dict]) -> List[Dict]:
    drv = _get_driver()
    try:
        for itm in news_list:
            drv.get(itm["url"]); 
            time.sleep(1.5)

            # ë³¸ë¬¸
            try:
                txt = drv.find_element(By.ID,"dic_area").text
                itm["content"]=" ".join(t.strip() for t in txt.split("\n") if t.strip())
            except: itm["content"]=""
            # ì…ë ¥ì‹œê°„
            try:
                raw = drv.find_element(
                    By.CSS_SELECTOR,
                    'span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME'
                ).get_attribute("data-date-time") # ì˜ˆ: "2025-02-28T12:23:00"

                # ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
                parsed_date = datetime.fromisoformat(raw)

                # ë‹¤ì‹œ ISO 8601 í˜•ì‹ìœ¼ë¡œ ë¬¸ìì—´ ë³€í™˜ (T í¬í•¨)
                iso_date = parsed_date.isoformat(timespec='seconds').replace(":", "-") # ğŸ‘‰ ê²°ê³¼: "2025-02-28T12:23:00"

                itm["published_at"] = iso_date
                logger.info("ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ")

            except: itm["published_at"]=""

            logger.debug(f"[ENRICH] idx={itm['index']} published_at={itm['published_at']}")
        logger.info("[CRAWL] enrichment done")
        return news_list
    finally: drv.quit()

if __name__ == "__main__":
    heads = crawl_headlines()
    full = enrich_articles(heads)

    import pandas as pd
    out = Path(f"./output/crawling/news_data_{datetime.utcnow():%Y%m%d}.csv")
    pd.DataFrame(full).to_csv(out, index=False, encoding="utf-8-sig")
    print(f"CSV saved â†’ {out.resolve()}")
