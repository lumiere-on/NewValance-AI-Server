"""
services/crawler.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ â†’ ì œëª©Â·URLÂ·ê¸°ì‚¬ ë³¸ë¬¸Â·ëŒ€í‘œì´ë¯¸ì§€Â·ê°ì •Â·ê¸°ì‚¬ì…ë ¥ì‹œê°„(published_at) í¬ë¡¤ë§
- í¬ë¡¬ ë“œë¼ì´ë²„ëŠ” chromedriverâ€‘autoinstaller ë¡œ ìë™ ì„¤ì¹˜
- headless ì˜µì…˜ ê¸°ë³¸
"""

from __future__ import annotations
import time, chromedriver_autoinstaller
from datetime import datetime
from pathlib import Path
from typing import List, Dict

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager 

from app.core.logger import logger

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTIONS = {
    "100": 1, #ì •ì¹˜
    "101": 2, #ê²½ì œ
    "102": 6, #ì‚¬íšŒ
    "103": 4, #ìƒí™œ/ë¬¸í™”
    "104": 3, #ì„¸ê³„
    "105": 5, #IT/ê³¼í•™
}

def _get_driver(headless: bool = True) -> webdriver.Chrome:
    chrome_options = webdriver.ChromeOptions()
    if headless:
        chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("lang=ko_KR")

    service = Service(ChromeDriverManager().install())  
    return webdriver.Chrome(options=chrome_options)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl_headlines(sections: List[str] | None = None) -> List[Dict]:
    """
    ì„¹ì…˜ë³„ í—¤ë“œë¼ì¸(ì œëª©+URL)ë§Œ ìš°ì„  ìˆ˜ì§‘
    """
    drv = _get_driver()
    date_str = datetime.utcnow().strftime("%Y%m%d")
    results: List[Dict] = []
    base = "https://news.naver.com/section/"
    try:
        idx = 0
        for sec in sections or SECTIONS.keys():
            drv.get(base + sec)
            time.sleep(1.5)
            for art in drv.find_elements(By.CSS_SELECTOR, "a.sa_text_title"):
                url = art.get_attribute("href")
                title = art.find_element(By.TAG_NAME, "strong").text.strip()
                results.append({
                    "index": idx,
                    "date": date_str,
                    "section": SECTIONS.get(sec, sec),
                    "url": url,
                    "title": title,
                })
                logger.debug(f"[HEAD] ({idx}) {title}")
                idx += 1
        logger.info(f"[CRAWL] headlines collected: {len(results)}")
        return results
    finally:
        drv.quit()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def enrich_articles(news_list: List[Dict]) -> List[Dict]:
    """
    ê°œë³„ ê¸°ì‚¬ URL ë¡œ ì´ë™í•´ ë³¸ë¬¸, ì…ë ¥ì‹œê°„ ì¶”ê°€
    """
    drv = _get_driver()
    try:
        for item in news_list:
            drv.get(item["url"])
            time.sleep(1.5)

            # â”€â”€ ë³¸ë¬¸
            try:
                txt = drv.find_element(By.ID, "dic_area").text
                item["content"] = " ".join(t.strip() for t in txt.split("\n") if t.strip())
                print("ë³¸ë¬¸ ë‚´ìš© í¬ë¡¤ë§ ì™„ë£Œ")
            except Exception:
                item["content"] = ""

            # â”€â”€ ê¸°ì‚¬ ì…ë ¥ì‹œê°„
            try:
                dt_tag = drv.find_element(
                    By.CSS_SELECTOR,
                    'span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME'
                )
                raw_date_str = dt_tag.get_attribute("data-date-time")  # ì˜ˆ: "2025-02-28T12:23:00"
    
                # ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
                parsed_date = datetime.fromisoformat(raw_date_str)

                # ë‹¤ì‹œ ISO 8601 í˜•ì‹ìœ¼ë¡œ ë¬¸ìì—´ ë³€í™˜ (T í¬í•¨)
                iso_date = parsed_date.isoformat()  # ğŸ‘‰ ê²°ê³¼: "2025-02-28T12:23:00"

                item["published_at"] = iso_date
                print("ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ")
            except Exception:
                item["published_at"] = ""

            logger.debug(f"[ENRICH] idx={item['index']} published_at={item['published_at']}")
        logger.info("[CRAWL] enrichment done")
        return news_list
    finally:
        drv.quit()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI í…ŒìŠ¤íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    heads = crawl_headlines()
    full = enrich_articles(heads)
    import pandas as pd, os
    out = Path(f"./output/crawling/news_data_{datetime.utcnow():%Y%m%d}.csv")
    pd.DataFrame(full).to_csv(out, index=False, encoding="utf-8-sig")
    print(f"CSV saved â†’ {out.resolve()}")
